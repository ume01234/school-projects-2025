# オセロ実験プラットフォーム構想（完全版）

## 全体コンセプト

日本人にとってルールが普遍的で理解しやすい「オセロ」を題材に、  
1つのWebプラットフォーム上に複数モードを実装し、対局ログとアンケートを継続的に収集する。

扱う主な実験軸は次の2つ：

- **HCI・認知科学の文脈**：盤面情報をあえて制限した「目隠しオセロ」で、認知負荷・UX・手の質の変化を調べる。
- **強化学習＋HCIの文脈**：勝利最大化AIと「接戦を目指す接待AI」を対比させ、人間のパフォーマンスと体験への影響を調べる。

この構成により、オセロを「人間・AI・UI・認知」を横串にした実験場として活用する。

---

## 実装するモード一覧

### 1. 通常オセロ（ベースライン）

- ルール：一般的なオセロ。盤面は常に完全可視。
- 対戦形態：
  - 人間 vs 人間
  - 人間 vs シンプルAI（弱めのルールベース or 探索浅めAI）
- 役割：
  - 他モードと比較するためのベースライン。
  - プレイヤーの実力・プレイスタイルを測る基準。

---

### 2. 目隠しオセロ（HCI・認知）

完全情報ゲームであるオセロに、UI側の制約として「部分観測」を導入するモード群。

#### 2.1 モード設計例

- **目隠し1（軽度）**  
  - 盤面の一部（例：1/4）を常に非表示にする。  
  - 自分の石が置ける合法手の位置はハイライト表示。
- **目隠し2（中程度）**  
  - 盤面の1/2程度を非表示。  
  - 最近打たれた数手のみ局所的に見えるなど、情報をさらに絞る。
- **目隠し3（重度）**  
  - 盤面は枠だけが見え、合法手の候補位置だけが表示される。  
  - 石の配置はプレイヤーが「記憶で補う」必要がある。

#### 2.2 追加ギミック：確認アクション

- 任意の手番で、
  - 「1手消費して、任意の小領域（例：3×3）を一時的に可視化」
  できるアクションを導入する案。
- これにより、
  - 記憶に頼るか
  - 手番を犠牲にして安全確認するか
  というメタ戦略が生まれ、ゲーム性と実験的面白さが増す。

#### 2.3 目標と測定

- 目的：
  - 可視情報量と確認行動が、
    - ミス率
    - 思考時間
    - 主観的負荷（疲労・難しさ）
    - 楽しさ・没入感
    にどう影響するかを調べる。
- 収集するデータ：
  - 対局ログ：着手列、着手時刻、勝敗、スコア差、確認アクション使用回数など。
  - アンケート：
    - 難しさ・疲労感・楽しさ・イライラ・もう一度やりたいか などをリッカート尺度で。
    - 普段のゲーム経験（ボードゲーム歴・ビデオゲーム歴など）も軽く取得。

---

### 3. 最強強化学習AIモード（ガチAI）

- 内容：
  - 強化学習またはミニマックス＋評価関数を用いた「強いオセロAI」と対戦するモード。
  - 既存の研究（Othello RL や Othello World Model 系）で使われている実装・手法の流用を前提にする。
- 役割：
  - 「人間 vs 強AI」のデータを集め、接待AIとの比較対象とする。
  - 人間が強AIに対してどう立ち向かうか（定石寄りの手を増やすか、投げやりになるか等）を観察する。

---

### 4. 接待AIモード（強化学習＋HCI）

#### 4.1 接待AIのコンセプト

- 目的は「勝利」ではなく「接戦」。
- 勝率50%前後、スコア差が小さい、逆転が起きやすい対局を目指す。
- 人間の主観的体験（楽しさ・緊張感・満足感）や上達への影響を調べる。

#### 4.2 接待AIの設計案

- 環境：通常のオセロルールをそのまま使用。
- 報酬設計例：
  - 最終スコア差 \(|score_{AI} - score_{human}|\) が小さいほど高報酬。
  - 一方的展開にならず、終盤まで均衡が続く対局にボーナス。
  - 必要に応じて、「人間が逆転可能な局面」を作る行動にボーナスを与える。
- 学習方法：
  - オフラインで自己対戦や固定強さの相手に対して強化学習。
  - 十分に振る舞いが安定した時点の方策を対戦用として固定。

#### 4.3 ガチAIとの比較実験

- 条件例：
  - ガチAIモード  
  - 接待AIモード
- 比較する指標：
  - 勝率・スコア差・悪手率
  - プレイ時間・継続意欲（「もう1局」のクリック率）
  - アンケート項目：
    - 楽しさ、緊張感、悔しさ、上達感、自分の実力を正しく測れた感覚、AIへの信頼感 など。

- バリエーション：
  - 「AIが接待していることを事前に説明する条件」 vs 「何も説明しない条件」を分け、  
    - AIの意図の知識がUXや信頼にどう影響するかも分析できる。

---

## Webプラットフォームとしての構成

### 共通UIとフロー

1. トップ画面：
   - モード選択（通常 / 目隠し1 / 目隠し2 / 目隠し3 / 最強AI / 接待AI）
2. 対局画面：
   - 盤面表示（モードに応じた目隠し処理を適用）
   - 手番ハイライト、合法手表示
   - 必要なら確認アクションボタン
3. 終了画面：
   - 対局の統計表示（勝敗・スコア差・手数・思考時間など）
   - その対局に対応したアンケートフォームを同一画面に表示
   - 送信ボタン1つで、対局IDと紐付けてサーバーに保存

### 保存するデータの基本構造

- 対局ID
- モード種別（目隠し1/2/3/ガチAI/接待AI/通常）
- プレイヤーID（匿名ハッシュ可）
- 着手ログ（手番ごとの位置・時間）
- 勝敗・スコア差・手数
- 目隠し関連：可視情報量、確認アクション使用回数など
- AI関連：対戦AI種別（ガチ/接待）、AI側内部評価値ログ（可能なら）
- アンケート回答（UX・負荷・信頼・上達感 等）

---

## 実装難易度と開発フロー（簡潔版）

### 難易度感

- オセロロジック・通常対戦：低
- 目隠しモード（表示制御のみ）：低〜中
- Web UI＋ログ・アンケート保存：低〜中
- 最強AI（既存実装の組み込み）：中
- 接待AI（報酬設計＋学習）：中〜やや高（ただしオセロなので問題サイズは小さめ）

### 段階的な開発フロー

1. **基盤実装**
   - 通常オセロ（人 vs 人 / 簡単AI）＋ログ保存。
2. **目隠しモード追加**
   - 目隠し1/2/3のUI制御を実装し、モード情報をログに付与。
3. **アンケート統合**
   - 終局画面に統計＋簡易アンケートフォームを表示し、対局IDと紐付けて保存。
4. **ガチAI導入**
   - 既存オセロAIを接続し、「最強AIモード」を追加。
5. **接待AI開発**
   - 報酬を「接戦」にしたRLエージェントを学習し、「接待AIモード」として組み込み。
6. **公開・データ収集・分析**
   - Web上で公開し、継続的にログとアンケートを集め、  
     - 目隠しオセロ（HCI・認知）  
     - 接待AI（RL＋HCI）  
     の2軸で順次解析・発表していく。

---

## 研究としての位置づけ

- **目隠しオセロ**：
  - 完全情報ゲームに意図的な部分観測を導入し、情報量と認知負荷・ミス・UXの関係を調べる実験。
  - 既存の「ボードゲーム×ワーキングメモリ」「UIと認知負荷」の文脈と自然に接続できる。

- **接待AI**：
  - 勝利ではなく接戦を目標とする対戦AIが、人間のパフォーマンス・体験・上達に与える影響を評価する実験。
  - 動的難易度調整（DDA）、human–AI interaction、human-likeness評価といった最近のトピックと親和性が高い。

1つのオセロプラットフォームで両方を扱うことで、  
「同じルール・同じUI基盤の上で、情報・AIの振る舞い・人間の側の体験を統一的に比較できる」  
というのが、この構想の強みとなる。
